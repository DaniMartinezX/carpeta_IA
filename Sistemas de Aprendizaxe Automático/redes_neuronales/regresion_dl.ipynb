{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "housing = fetch_california_housing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _california_housing_dataset:\n",
      "\n",
      "California Housing dataset\n",
      "--------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      ":Number of Instances: 20640\n",
      "\n",
      ":Number of Attributes: 8 numeric, predictive attributes and the target\n",
      "\n",
      ":Attribute Information:\n",
      "    - MedInc        median income in block group\n",
      "    - HouseAge      median house age in block group\n",
      "    - AveRooms      average number of rooms per household\n",
      "    - AveBedrms     average number of bedrooms per household\n",
      "    - Population    block group population\n",
      "    - AveOccup      average number of household members\n",
      "    - Latitude      block group latitude\n",
      "    - Longitude     block group longitude\n",
      "\n",
      ":Missing Attribute Values: None\n",
      "\n",
      "This dataset was obtained from the StatLib repository.\n",
      "https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\n",
      "\n",
      "The target variable is the median house value for California districts,\n",
      "expressed in hundreds of thousands of dollars ($100,000).\n",
      "\n",
      "This dataset was derived from the 1990 U.S. census, using one row per census\n",
      "block group. A block group is the smallest geographical unit for which the U.S.\n",
      "Census Bureau publishes sample data (a block group typically has a population\n",
      "of 600 to 3,000 people).\n",
      "\n",
      "A household is a group of people residing within a home. Since the average\n",
      "number of rooms and bedrooms in this dataset are provided per household, these\n",
      "columns may take surprisingly large values for block groups with few households\n",
      "and many empty houses, such as vacation resorts.\n",
      "\n",
      "It can be downloaded/loaded using the\n",
      ":func:`sklearn.datasets.fetch_california_housing` function.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n",
      "      Statistics and Probability Letters, 33 (1997) 291-297\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(housing[\"DESCR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = housing.data\n",
    "y = housing.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separación de los datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los datos de las X deben estar escaladas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val =   scaler.transform(X_val)\n",
    "X_test =  scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.673, 3.158, 2.583, ..., 0.729, 2.765, 1.566])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11610, 8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación de una red neuronal (modelo)\n",
    "Función secuencial que le pasamos una lista de capas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Api funcional\n",
    "inputs = keras.Input((8,))\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(inputs)\n",
    "outputs = keras.layers.Dense(1)(hidden1)\n",
    "model = keras.models.Model(inputs=[inputs], outputs=[outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Input(shape=(8,)), # Capa de entrada de 8 \n",
    "    # Otra forma para ahorrar las 2 primeras líneas de la creación de la red neuronal:\n",
    "    # keras.layers.Dense(30, activation='relu', input_shape=(8,)), Tiene una capa de entrada que tiene 8 neuronas\n",
    "    keras.layers.Dense(30, activation='relu'),\n",
    "    keras.layers.Dense(1)   # Capa de salida (un valor de salida)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para decirle la **función de error** que va utilizar hay que compilar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error Medio Cuadrático\n",
    "model.compile(loss=\"mean_squared_error\", \n",
    "              optimizer = keras.optimizers.SGD(learning_rate=0.001))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El **optimizador** es para gestionar los pesos de las neuronas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para enseñar lo que tiene:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">270</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m)             │           \u001b[38;5;34m270\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m31\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">301</span> (1.18 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m301\u001b[0m (1.18 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">301</span> (1.18 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m301\u001b[0m (1.18 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 3.5654 - val_loss: 1.2447\n",
      "Epoch 2/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 923us/step - loss: 0.9381 - val_loss: 0.8285\n",
      "Epoch 3/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 821us/step - loss: 0.7637 - val_loss: 0.7411\n",
      "Epoch 4/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 824us/step - loss: 0.7041 - val_loss: 0.6902\n",
      "Epoch 5/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 864us/step - loss: 0.6595 - val_loss: 0.6495\n",
      "Epoch 6/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 827us/step - loss: 0.6221 - val_loss: 0.6151\n",
      "Epoch 7/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 856us/step - loss: 0.5906 - val_loss: 0.5864\n",
      "Epoch 8/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 875us/step - loss: 0.5641 - val_loss: 0.5624\n",
      "Epoch 9/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 890us/step - loss: 0.5420 - val_loss: 0.5419\n",
      "Epoch 10/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 867us/step - loss: 0.5233 - val_loss: 0.5248\n",
      "Epoch 11/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 903us/step - loss: 0.5075 - val_loss: 0.5101\n",
      "Epoch 12/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 819us/step - loss: 0.4939 - val_loss: 0.4978\n",
      "Epoch 13/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 873us/step - loss: 0.4823 - val_loss: 0.4868\n",
      "Epoch 14/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 818us/step - loss: 0.4722 - val_loss: 0.4773\n",
      "Epoch 15/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 860us/step - loss: 0.4635 - val_loss: 0.4690\n",
      "Epoch 16/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 873us/step - loss: 0.4561 - val_loss: 0.4620\n",
      "Epoch 17/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 897us/step - loss: 0.4496 - val_loss: 0.4558\n",
      "Epoch 18/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 869us/step - loss: 0.4439 - val_loss: 0.4504\n",
      "Epoch 19/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 824us/step - loss: 0.4388 - val_loss: 0.4456\n",
      "Epoch 20/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 883us/step - loss: 0.4343 - val_loss: 0.4412\n",
      "Epoch 21/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 881us/step - loss: 0.4302 - val_loss: 0.4371\n",
      "Epoch 22/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 908us/step - loss: 0.4264 - val_loss: 0.4334\n",
      "Epoch 23/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 843us/step - loss: 0.4230 - val_loss: 0.4299\n",
      "Epoch 24/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 797us/step - loss: 0.4198 - val_loss: 0.4267\n",
      "Epoch 25/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 805us/step - loss: 0.4168 - val_loss: 0.4236\n",
      "Epoch 26/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 825us/step - loss: 0.4140 - val_loss: 0.4208\n",
      "Epoch 27/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 874us/step - loss: 0.4114 - val_loss: 0.4183\n",
      "Epoch 28/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 860us/step - loss: 0.4090 - val_loss: 0.4160\n",
      "Epoch 29/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 872us/step - loss: 0.4067 - val_loss: 0.4138\n",
      "Epoch 30/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 895us/step - loss: 0.4046 - val_loss: 0.4118\n",
      "Epoch 31/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 865us/step - loss: 0.4027 - val_loss: 0.4099\n",
      "Epoch 32/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 777us/step - loss: 0.4008 - val_loss: 0.4081\n",
      "Epoch 33/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 861us/step - loss: 0.3990 - val_loss: 0.4065\n",
      "Epoch 34/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 835us/step - loss: 0.3974 - val_loss: 0.4049\n",
      "Epoch 35/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 767us/step - loss: 0.3958 - val_loss: 0.4034\n",
      "Epoch 36/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 855us/step - loss: 0.3944 - val_loss: 0.4020\n",
      "Epoch 37/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 822us/step - loss: 0.3930 - val_loss: 0.4006\n",
      "Epoch 38/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 860us/step - loss: 0.3916 - val_loss: 0.3994\n",
      "Epoch 39/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 858us/step - loss: 0.3904 - val_loss: 0.3982\n",
      "Epoch 40/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 885us/step - loss: 0.3892 - val_loss: 0.3971\n",
      "Epoch 41/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 892us/step - loss: 0.3881 - val_loss: 0.3960\n",
      "Epoch 42/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 924us/step - loss: 0.3870 - val_loss: 0.3950\n",
      "Epoch 43/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 876us/step - loss: 0.3860 - val_loss: 0.3941\n",
      "Epoch 44/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 859us/step - loss: 0.3851 - val_loss: 0.3932\n",
      "Epoch 45/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 859us/step - loss: 0.3841 - val_loss: 0.3924\n",
      "Epoch 46/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 829us/step - loss: 0.3832 - val_loss: 0.3916\n",
      "Epoch 47/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 892us/step - loss: 0.3824 - val_loss: 0.3909\n",
      "Epoch 48/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 875us/step - loss: 0.3815 - val_loss: 0.3901\n",
      "Epoch 49/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 875us/step - loss: 0.3807 - val_loss: 0.3895\n",
      "Epoch 50/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 891us/step - loss: 0.3799 - val_loss: 0.3890\n",
      "Epoch 51/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 893us/step - loss: 0.3790 - val_loss: 0.3886\n",
      "Epoch 52/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 874us/step - loss: 0.3782 - val_loss: 0.3881\n",
      "Epoch 53/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 843us/step - loss: 0.3774 - val_loss: 0.3876\n",
      "Epoch 54/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 847us/step - loss: 0.3767 - val_loss: 0.3869\n",
      "Epoch 55/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 873us/step - loss: 0.3760 - val_loss: 0.3864\n",
      "Epoch 56/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 845us/step - loss: 0.3754 - val_loss: 0.3858\n",
      "Epoch 57/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 812us/step - loss: 0.3747 - val_loss: 0.3852\n",
      "Epoch 58/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 875us/step - loss: 0.3741 - val_loss: 0.3846\n",
      "Epoch 59/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 863us/step - loss: 0.3734 - val_loss: 0.3840\n",
      "Epoch 60/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 829us/step - loss: 0.3728 - val_loss: 0.3834\n",
      "Epoch 61/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 783us/step - loss: 0.3722 - val_loss: 0.3828\n",
      "Epoch 62/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 919us/step - loss: 0.3716 - val_loss: 0.3824\n",
      "Epoch 63/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 829us/step - loss: 0.3710 - val_loss: 0.3819\n",
      "Epoch 64/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 783us/step - loss: 0.3703 - val_loss: 0.3815\n",
      "Epoch 65/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 821us/step - loss: 0.3697 - val_loss: 0.3810\n",
      "Epoch 66/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 830us/step - loss: 0.3691 - val_loss: 0.3805\n",
      "Epoch 67/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 800us/step - loss: 0.3685 - val_loss: 0.3799\n",
      "Epoch 68/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 829us/step - loss: 0.3679 - val_loss: 0.3794\n",
      "Epoch 69/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 783us/step - loss: 0.3674 - val_loss: 0.3789\n",
      "Epoch 70/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 826us/step - loss: 0.3668 - val_loss: 0.3784\n",
      "Epoch 71/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 830us/step - loss: 0.3663 - val_loss: 0.3779\n",
      "Epoch 72/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 830us/step - loss: 0.3658 - val_loss: 0.3773\n",
      "Epoch 73/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 829us/step - loss: 0.3653 - val_loss: 0.3768\n",
      "Epoch 74/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 921us/step - loss: 0.3648 - val_loss: 0.3763\n",
      "Epoch 75/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 830us/step - loss: 0.3644 - val_loss: 0.3757\n",
      "Epoch 76/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 829us/step - loss: 0.3639 - val_loss: 0.3754\n",
      "Epoch 77/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 828us/step - loss: 0.3634 - val_loss: 0.3749\n",
      "Epoch 78/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 875us/step - loss: 0.3630 - val_loss: 0.3745\n",
      "Epoch 79/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 968us/step - loss: 0.3625 - val_loss: 0.3740\n",
      "Epoch 80/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 829us/step - loss: 0.3621 - val_loss: 0.3736\n",
      "Epoch 81/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 832us/step - loss: 0.3617 - val_loss: 0.3733\n",
      "Epoch 82/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 875us/step - loss: 0.3612 - val_loss: 0.3728\n",
      "Epoch 83/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 829us/step - loss: 0.3608 - val_loss: 0.3724\n",
      "Epoch 84/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 848us/step - loss: 0.3604 - val_loss: 0.3720\n",
      "Epoch 85/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 875us/step - loss: 0.3600 - val_loss: 0.3717\n",
      "Epoch 86/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 829us/step - loss: 0.3596 - val_loss: 0.3713\n",
      "Epoch 87/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 829us/step - loss: 0.3592 - val_loss: 0.3709\n",
      "Epoch 88/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 879us/step - loss: 0.3588 - val_loss: 0.3705\n",
      "Epoch 89/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 829us/step - loss: 0.3584 - val_loss: 0.3701\n",
      "Epoch 90/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 786us/step - loss: 0.3581 - val_loss: 0.3698\n",
      "Epoch 91/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 783us/step - loss: 0.3577 - val_loss: 0.3694\n",
      "Epoch 92/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 829us/step - loss: 0.3574 - val_loss: 0.3690\n",
      "Epoch 93/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 840us/step - loss: 0.3570 - val_loss: 0.3687\n",
      "Epoch 94/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 877us/step - loss: 0.3567 - val_loss: 0.3682\n",
      "Epoch 95/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 829us/step - loss: 0.3562 - val_loss: 0.3675\n",
      "Epoch 96/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 856us/step - loss: 0.3558 - val_loss: 0.3669\n",
      "Epoch 97/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 826us/step - loss: 0.3554 - val_loss: 0.3664\n",
      "Epoch 98/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 829us/step - loss: 0.3550 - val_loss: 0.3659\n",
      "Epoch 99/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 819us/step - loss: 0.3547 - val_loss: 0.3655\n",
      "Epoch 100/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 876us/step - loss: 0.3544 - val_loss: 0.3650\n",
      "Epoch 101/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 857us/step - loss: 0.3541 - val_loss: 0.3646\n",
      "Epoch 102/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 832us/step - loss: 0.3538 - val_loss: 0.3643\n",
      "Epoch 103/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 829us/step - loss: 0.3535 - val_loss: 0.3639\n",
      "Epoch 104/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 847us/step - loss: 0.3532 - val_loss: 0.3636\n",
      "Epoch 105/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 883us/step - loss: 0.3530 - val_loss: 0.3633\n",
      "Epoch 106/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 885us/step - loss: 0.3527 - val_loss: 0.3630\n",
      "Epoch 107/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 812us/step - loss: 0.3524 - val_loss: 0.3627\n",
      "Epoch 108/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 830us/step - loss: 0.3522 - val_loss: 0.3624\n",
      "Epoch 109/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 820us/step - loss: 0.3519 - val_loss: 0.3621\n",
      "Epoch 110/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 897us/step - loss: 0.3517 - val_loss: 0.3618\n",
      "Epoch 111/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 859us/step - loss: 0.3514 - val_loss: 0.3616\n",
      "Epoch 112/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 916us/step - loss: 0.3512 - val_loss: 0.3614\n",
      "Epoch 113/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 829us/step - loss: 0.3510 - val_loss: 0.3611\n",
      "Epoch 114/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 829us/step - loss: 0.3507 - val_loss: 0.3609\n",
      "Epoch 115/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 774us/step - loss: 0.3505 - val_loss: 0.3607\n",
      "Epoch 116/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 829us/step - loss: 0.3503 - val_loss: 0.3605\n",
      "Epoch 117/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 812us/step - loss: 0.3500 - val_loss: 0.3603\n",
      "Epoch 118/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 829us/step - loss: 0.3498 - val_loss: 0.3601\n",
      "Epoch 119/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 830us/step - loss: 0.3495 - val_loss: 0.3599\n",
      "Epoch 120/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 829us/step - loss: 0.3493 - val_loss: 0.3597\n",
      "Epoch 121/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 876us/step - loss: 0.3491 - val_loss: 0.3595\n",
      "Epoch 122/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 828us/step - loss: 0.3488 - val_loss: 0.3593\n",
      "Epoch 123/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 829us/step - loss: 0.3485 - val_loss: 0.3591\n",
      "Epoch 124/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 905us/step - loss: 0.3483 - val_loss: 0.3589\n",
      "Epoch 125/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 783us/step - loss: 0.3481 - val_loss: 0.3587\n",
      "Epoch 126/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 875us/step - loss: 0.3478 - val_loss: 0.3585\n",
      "Epoch 127/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 829us/step - loss: 0.3476 - val_loss: 0.3583\n",
      "Epoch 128/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 830us/step - loss: 0.3473 - val_loss: 0.3581\n",
      "Epoch 129/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 829us/step - loss: 0.3471 - val_loss: 0.3580\n",
      "Epoch 130/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 921us/step - loss: 0.3469 - val_loss: 0.3578\n",
      "Epoch 131/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 829us/step - loss: 0.3467 - val_loss: 0.3577\n",
      "Epoch 132/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 875us/step - loss: 0.3464 - val_loss: 0.3575\n",
      "Epoch 133/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 826us/step - loss: 0.3462 - val_loss: 0.3574\n",
      "Epoch 134/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 836us/step - loss: 0.3460 - val_loss: 0.3572\n",
      "Epoch 135/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 829us/step - loss: 0.3458 - val_loss: 0.3571\n",
      "Epoch 136/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 893us/step - loss: 0.3456 - val_loss: 0.3570\n",
      "Epoch 137/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 837us/step - loss: 0.3454 - val_loss: 0.3568\n",
      "Epoch 138/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 876us/step - loss: 0.3452 - val_loss: 0.3567\n",
      "Epoch 139/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 859us/step - loss: 0.3450 - val_loss: 0.3565\n",
      "Epoch 140/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 876us/step - loss: 0.3448 - val_loss: 0.3564\n",
      "Epoch 141/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 829us/step - loss: 0.3446 - val_loss: 0.3563\n",
      "Epoch 142/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 879us/step - loss: 0.3444 - val_loss: 0.3562\n",
      "Epoch 143/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 876us/step - loss: 0.3442 - val_loss: 0.3561\n",
      "Epoch 144/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 875us/step - loss: 0.3440 - val_loss: 0.3560\n",
      "Epoch 145/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 863us/step - loss: 0.3438 - val_loss: 0.3559\n",
      "Epoch 146/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 829us/step - loss: 0.3436 - val_loss: 0.3558\n",
      "Epoch 147/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 957us/step - loss: 0.3434 - val_loss: 0.3557\n",
      "Epoch 148/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 829us/step - loss: 0.3432 - val_loss: 0.3556\n",
      "Epoch 149/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 830us/step - loss: 0.3430 - val_loss: 0.3555\n",
      "Epoch 150/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 859us/step - loss: 0.3429 - val_loss: 0.3554\n",
      "Epoch 151/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 830us/step - loss: 0.3427 - val_loss: 0.3553\n",
      "Epoch 152/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 875us/step - loss: 0.3425 - val_loss: 0.3552\n",
      "Epoch 153/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 826us/step - loss: 0.3423 - val_loss: 0.3551\n",
      "Epoch 154/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 876us/step - loss: 0.3422 - val_loss: 0.3550\n",
      "Epoch 155/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 893us/step - loss: 0.3420 - val_loss: 0.3549\n",
      "Epoch 156/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 846us/step - loss: 0.3418 - val_loss: 0.3548\n",
      "Epoch 157/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 922us/step - loss: 0.3416 - val_loss: 0.3547\n",
      "Epoch 158/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 996us/step - loss: 0.3415 - val_loss: 0.3547\n",
      "Epoch 159/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 829us/step - loss: 0.3413 - val_loss: 0.3546\n",
      "Epoch 160/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 866us/step - loss: 0.3411 - val_loss: 0.3545\n",
      "Epoch 161/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 831us/step - loss: 0.3409 - val_loss: 0.3544\n",
      "Epoch 162/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 893us/step - loss: 0.3408 - val_loss: 0.3543\n",
      "Epoch 163/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 872us/step - loss: 0.3406 - val_loss: 0.3543\n",
      "Epoch 164/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 875us/step - loss: 0.3404 - val_loss: 0.3542\n",
      "Epoch 165/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 865us/step - loss: 0.3403 - val_loss: 0.3541\n",
      "Epoch 166/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 868us/step - loss: 0.3401 - val_loss: 0.3541\n",
      "Epoch 167/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 870us/step - loss: 0.3399 - val_loss: 0.3540\n",
      "Epoch 168/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 876us/step - loss: 0.3398 - val_loss: 0.3540\n",
      "Epoch 169/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 942us/step - loss: 0.3396 - val_loss: 0.3539\n",
      "Epoch 170/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 829us/step - loss: 0.3394 - val_loss: 0.3538\n",
      "Epoch 171/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 829us/step - loss: 0.3393 - val_loss: 0.3538\n",
      "Epoch 172/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 829us/step - loss: 0.3391 - val_loss: 0.3537\n",
      "Epoch 173/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 830us/step - loss: 0.3390 - val_loss: 0.3537\n",
      "Epoch 174/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 811us/step - loss: 0.3388 - val_loss: 0.3536\n",
      "Epoch 175/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 814us/step - loss: 0.3387 - val_loss: 0.3535\n",
      "Epoch 176/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 829us/step - loss: 0.3385 - val_loss: 0.3534\n",
      "Epoch 177/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 828us/step - loss: 0.3383 - val_loss: 0.3534\n",
      "Epoch 178/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 829us/step - loss: 0.3382 - val_loss: 0.3533\n",
      "Epoch 179/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 783us/step - loss: 0.3380 - val_loss: 0.3532\n",
      "Epoch 180/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 826us/step - loss: 0.3379 - val_loss: 0.3532\n",
      "Epoch 181/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 876us/step - loss: 0.3377 - val_loss: 0.3531\n",
      "Epoch 182/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 834us/step - loss: 0.3376 - val_loss: 0.3530\n",
      "Epoch 183/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 920us/step - loss: 0.3374 - val_loss: 0.3530\n",
      "Epoch 184/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 895us/step - loss: 0.3373 - val_loss: 0.3529\n",
      "Epoch 185/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 830us/step - loss: 0.3371 - val_loss: 0.3529\n",
      "Epoch 186/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 829us/step - loss: 0.3370 - val_loss: 0.3528\n",
      "Epoch 187/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 833us/step - loss: 0.3368 - val_loss: 0.3527\n",
      "Epoch 188/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 829us/step - loss: 0.3367 - val_loss: 0.3526\n",
      "Epoch 189/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 829us/step - loss: 0.3365 - val_loss: 0.3526\n",
      "Epoch 190/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 889us/step - loss: 0.3364 - val_loss: 0.3525\n",
      "Epoch 191/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 829us/step - loss: 0.3363 - val_loss: 0.3524\n",
      "Epoch 192/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 829us/step - loss: 0.3361 - val_loss: 0.3524\n",
      "Epoch 193/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 875us/step - loss: 0.3360 - val_loss: 0.3523\n",
      "Epoch 194/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 830us/step - loss: 0.3358 - val_loss: 0.3522\n",
      "Epoch 195/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 830us/step - loss: 0.3357 - val_loss: 0.3521\n",
      "Epoch 196/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 829us/step - loss: 0.3355 - val_loss: 0.3521\n",
      "Epoch 197/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 783us/step - loss: 0.3354 - val_loss: 0.3520\n",
      "Epoch 198/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 767us/step - loss: 0.3353 - val_loss: 0.3519\n",
      "Epoch 199/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 830us/step - loss: 0.3351 - val_loss: 0.3518\n",
      "Epoch 200/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 829us/step - loss: 0.3350 - val_loss: 0.3517\n",
      "Epoch 201/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 877us/step - loss: 0.3349 - val_loss: 0.3516\n",
      "Epoch 202/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 927us/step - loss: 0.3348 - val_loss: 0.3515\n",
      "Epoch 203/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3346 - val_loss: 0.3515\n",
      "Epoch 204/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3345 - val_loss: 0.3513\n",
      "Epoch 205/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 891us/step - loss: 0.3344 - val_loss: 0.3513\n",
      "Epoch 206/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 863us/step - loss: 0.3342 - val_loss: 0.3512\n",
      "Epoch 207/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 903us/step - loss: 0.3341 - val_loss: 0.3512\n",
      "Epoch 208/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 842us/step - loss: 0.3339 - val_loss: 0.3511\n",
      "Epoch 209/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 846us/step - loss: 0.3338 - val_loss: 0.3511\n",
      "Epoch 210/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3337 - val_loss: 0.3510\n",
      "Epoch 211/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 913us/step - loss: 0.3335 - val_loss: 0.3510\n",
      "Epoch 212/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 947us/step - loss: 0.3334 - val_loss: 0.3510\n",
      "Epoch 213/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 883us/step - loss: 0.3333 - val_loss: 0.3510\n",
      "Epoch 214/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 897us/step - loss: 0.3331 - val_loss: 0.3509\n",
      "Epoch 215/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 968us/step - loss: 0.3330 - val_loss: 0.3509\n",
      "Epoch 216/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 944us/step - loss: 0.3328 - val_loss: 0.3508\n",
      "Epoch 217/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 924us/step - loss: 0.3327 - val_loss: 0.3508\n",
      "Epoch 218/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 830us/step - loss: 0.3325 - val_loss: 0.3508\n",
      "Epoch 219/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 862us/step - loss: 0.3323 - val_loss: 0.3507\n",
      "Epoch 220/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 955us/step - loss: 0.3322 - val_loss: 0.3507\n",
      "Epoch 221/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 868us/step - loss: 0.3320 - val_loss: 0.3507\n",
      "Epoch 222/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 935us/step - loss: 0.3318 - val_loss: 0.3506\n",
      "Epoch 223/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 856us/step - loss: 0.3317 - val_loss: 0.3506\n",
      "Epoch 224/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 963us/step - loss: 0.3315 - val_loss: 0.3505\n",
      "Epoch 225/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 854us/step - loss: 0.3313 - val_loss: 0.3505\n",
      "Epoch 226/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 911us/step - loss: 0.3312 - val_loss: 0.3504\n",
      "Epoch 227/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 858us/step - loss: 0.3310 - val_loss: 0.3504\n",
      "Epoch 228/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 919us/step - loss: 0.3308 - val_loss: 0.3503\n",
      "Epoch 229/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 863us/step - loss: 0.3307 - val_loss: 0.3503\n",
      "Epoch 230/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 828us/step - loss: 0.3305 - val_loss: 0.3502\n",
      "Epoch 231/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 933us/step - loss: 0.3304 - val_loss: 0.3502\n",
      "Epoch 232/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 876us/step - loss: 0.3303 - val_loss: 0.3501\n",
      "Epoch 233/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 853us/step - loss: 0.3301 - val_loss: 0.3501\n",
      "Epoch 234/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 873us/step - loss: 0.3300 - val_loss: 0.3500\n",
      "Epoch 235/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 916us/step - loss: 0.3298 - val_loss: 0.3500\n",
      "Epoch 236/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 822us/step - loss: 0.3297 - val_loss: 0.3500\n",
      "Epoch 237/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 826us/step - loss: 0.3295 - val_loss: 0.3500\n",
      "Epoch 238/10000\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 951us/step - loss: 0.3293 - val_loss: 0.3500\n"
     ]
    }
   ],
   "source": [
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.keras\", save_best_only=True)\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n",
    "history = model.fit(X_train, y_train, epochs=10000,\n",
    "                    callbacks=[checkpoint_cb, early_stopping_cb],\n",
    "                    validation_data=(X_val, y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En caso de tener valores de validación se los ponemos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(X_train, y_train, epochs=20, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x223d3490bf0>,\n",
       " <matplotlib.lines.Line2D at 0x223d1caa210>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(pd.DataFrame(model.history.history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 573us/step - loss: 0.3583\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3643883466720581"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_test = model.evaluate(X_test, y_test)\n",
    "mse_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.7425575],\n",
       "       [2.0484402],\n",
       "       [0.7951943]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_nueva = X_test[:3]\n",
    "y_nueva = model.predict(X_nueva)\n",
    "y_nueva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "FailedPreconditionError",
     "evalue": "{{function_node __wrapped__CreateSummaryFileWriter_device_/job:localhost/replica:0/task:0/device:CPU:0}} . is not a directory [Op:CreateSummaryFileWriter] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m checkpoint_cb \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mModelCheckpoint(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy_keras_model.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m, save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      6\u001b[0m tensorboard_cb \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mTensorBoard(root_logdir)\n\u001b[1;32m----> 7\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint_cb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensorboard_cb\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\daniel.martinezcarre\\AppData\\Local\\miniconda3\\envs\\deeplearning\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\daniel.martinezcarre\\AppData\\Local\\miniconda3\\envs\\deeplearning\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:5983\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   5981\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[0;32m   5982\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m-> 5983\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m: {{function_node __wrapped__CreateSummaryFileWriter_device_/job:localhost/replica:0/task:0/device:CPU:0}} . is not a directory [Op:CreateSummaryFileWriter] name: "
     ]
    }
   ],
   "source": [
    "#TensorBoard\n",
    "root_logdir = os.path.join(\".\", \"my_logs\")\n",
    "#root_logdir = \"./my_logs\"\n",
    "\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.keras\", save_best_only=True)\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(root_logdir)\n",
    "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_val, y_val),\n",
    "                    callbacks=[checkpoint_cb, tensorboard_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Si se está en una terminal:\n",
    "    tensorboard --lodir = ./my_logs --port = 6006\n",
    "\n",
    "    Si se está en un colab:\n",
    "    %load_ext tensorboard\n",
    "    %tensorboard --logdir = ./my_logs --port = 6006\n",
    "\"\"\"\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir = ./my_logs --port = 6006"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
