{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "426\n"
     ]
    }
   ],
   "source": [
    "texto = \"\"\"Al igual que la Computaci칩n comienza su andadura como respuesta a la pregunta sobre las posibilidades de\n",
    "automatizaci칩n de los procedimientos matem치ticos, la IA surge de forma natural al plantear hasta qu칠 punto aquellos\n",
    "procesos que consideramos propios de una inteligencia natural (en particular, la humana) podr칤an ser automatizables\n",
    "en mecanismos artificiales, tengan 칠stos un componente f칤sico concreto o s칩lo conceptual.\"\"\"\n",
    "\n",
    "print(len(texto))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '(', ')', ',', '.', 'A', 'C', 'I', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'z', '치', '칠', '칤', '칩']\n"
     ]
    }
   ],
   "source": [
    "vocabulario = sorted(list(set(texto)))\n",
    "print(vocabulario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulario: \n",
      " (),.ACIabcdefghilmnopqrstuz치칠칤칩 tama침o: 33\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocabulario: {''.join(vocabulario)} tama침o: {len(vocabulario)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '(': 2, ')': 3, ',': 4, '.': 5, 'A': 6, 'C': 7, 'I': 8, 'a': 9, 'b': 10, 'c': 11, 'd': 12, 'e': 13, 'f': 14, 'g': 15, 'h': 16, 'i': 17, 'l': 18, 'm': 19, 'n': 20, 'o': 21, 'p': 22, 'q': 23, 'r': 24, 's': 25, 't': 26, 'u': 27, 'z': 28, '치': 29, '칠': 30, '칤': 31, '칩': 32}\n",
      "{0: '\\n', 1: ' ', 2: '(', 3: ')', 4: ',', 5: '.', 6: 'A', 7: 'C', 8: 'I', 9: 'a', 10: 'b', 11: 'c', 12: 'd', 13: 'e', 14: 'f', 15: 'g', 16: 'h', 17: 'i', 18: 'l', 19: 'm', 20: 'n', 21: 'o', 22: 'p', 23: 'q', 24: 'r', 25: 's', 26: 't', 27: 'u', 28: 'z', 29: '치', 30: '칠', 31: '칤', 32: '칩'}\n"
     ]
    }
   ],
   "source": [
    "stoi = {ch:i for i,ch in enumerate(vocabulario)}\n",
    "print(stoi)\n",
    "itos = {i:ch for i,ch in enumerate(vocabulario)}\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokeniza(s):\n",
    "    ret = [stoi[c] for c in s]\n",
    "    return ret\n",
    "def destokeniza(ltokens):\n",
    "    ret = \"\".join([itos[i] for i in ltokens])\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22, 13, 22, 17, 26, 21]\n",
      "pepito\n"
     ]
    }
   ],
   "source": [
    "print(tokeniza(\"pepito\"))\n",
    "print(destokeniza([22, 13, 22, 17, 26, 21]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['쮺u치nto', 'tiempo', 'pas칩', 'desde', 'que', 'com칤', 'una', 'manzana?']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "tk = WhitespaceTokenizer()\n",
    "texto = \"쮺u치nto tiempo pas칩 desde que com칤 una manzana?\"\n",
    "texto_tokenizado = tk.tokenize(texto)\n",
    "print(texto_tokenizado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'Cu치nto', 'tiempo', 'pas칩', 'desde', 'que', 'com칤', 'una', 'manzana', '?']\n",
      "['쮺u치nto', 'tiempo', 'pas칩', 'desde', 'que', 'com칤', 'una', 'manzana', '?']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "texto = \"쮺u치nto tiempo pas칩 desde que com칤 una manzana?\"\n",
    "texto_tokenizado1 = WordPunctTokenizer().tokenize(texto)\n",
    "texto_tokenizado2 = TreebankWordTokenizer().tokenize(texto)\n",
    "print(texto_tokenizado1)\n",
    "print(texto_tokenizado2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'stanza'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\daniel.martinezcarre\\Desktop\\Repositorios\\ia_repositorio\\Programacion de IA\\tirmestre_2\\tokenizacion.ipynb Celda 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/daniel.martinezcarre/Desktop/Repositorios/ia_repositorio/Programacion%20de%20IA/tirmestre_2/tokenizacion.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mstanza\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/daniel.martinezcarre/Desktop/Repositorios/ia_repositorio/Programacion%20de%20IA/tirmestre_2/tokenizacion.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m stanza\u001b[39m.\u001b[39mdownload(\u001b[39m\"\u001b[39m\u001b[39mes\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/daniel.martinezcarre/Desktop/Repositorios/ia_repositorio/Programacion%20de%20IA/tirmestre_2/tokenizacion.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m nlp \u001b[39m=\u001b[39m stanza\u001b[39m.\u001b[39mPipeline(lang\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mes\u001b[39m\u001b[39m'\u001b[39m, processors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtokenize,mwt,pos,lemma\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'stanza'"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "stanza.download(\"es\")\n",
    "nlp = stanza.Pipeline(lang='es', processors='tokenize,mwt,pos,lemma')\n",
    "texto_pablito = \"Pablito clav칩 un clavito cuantos clavitos clava pablito\"\n",
    "doc = nlp(texto_pablito)\n",
    "print(*[f'Palabra: {word.text+\" \"}\\tLemma: {word.lemma}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token bueno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.trainers import BpeTrainer\n",
    "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "tokenizer.pre_tokenizer = Whitespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [f\"wikitext-103-raw-v1/wikitext-103-raw/wiki.{split}.raw\" for split in [\"test\", \"train\", \"valid\"]]\n",
    "tokenizer.train(files, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"tokenizer-wiki.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_file(\"tokenizer-wiki.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tokenizer.encode(\"Hello, y'all! How are you 游때 ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'y', \"'\", 'all', '!', 'How', 'are', 'you', '[UNK]', '?']\n"
     ]
    }
   ],
   "source": [
    "print(output.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27253, 16, 93, 11, 5097, 5, 7961, 5112, 6218, 0, 35]\n"
     ]
    }
   ],
   "source": [
    "print(output.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26, 27)\n"
     ]
    }
   ],
   "source": [
    "print(output.offsets[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'游때'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Hello, y'all! How are you 游때 ?\"\n",
    "sentence[26:27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.token_to_id(\"[SEP]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.processors import TemplateProcessing\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),\n",
    "        (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),\n",
    "    ],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
