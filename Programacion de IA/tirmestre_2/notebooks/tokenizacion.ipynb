{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "426\n"
     ]
    }
   ],
   "source": [
    "texto = \"\"\"Al igual que la Computación comienza su andadura como respuesta a la pregunta sobre las posibilidades de\n",
    "automatización de los procedimientos matemáticos, la IA surge de forma natural al plantear hasta qué punto aquellos\n",
    "procesos que consideramos propios de una inteligencia natural (en particular, la humana) podrían ser automatizables\n",
    "en mecanismos artificiales, tengan éstos un componente físico concreto o sólo conceptual.\"\"\"\n",
    "\n",
    "print(len(texto))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '(', ')', ',', '.', 'A', 'C', 'I', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'z', 'á', 'é', 'í', 'ó']\n"
     ]
    }
   ],
   "source": [
    "vocabulario = sorted(list(set(texto)))\n",
    "print(vocabulario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulario: \n",
      " (),.ACIabcdefghilmnopqrstuzáéíó tamaño: 33\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocabulario: {''.join(vocabulario)} tamaño: {len(vocabulario)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '(': 2, ')': 3, ',': 4, '.': 5, 'A': 6, 'C': 7, 'I': 8, 'a': 9, 'b': 10, 'c': 11, 'd': 12, 'e': 13, 'f': 14, 'g': 15, 'h': 16, 'i': 17, 'l': 18, 'm': 19, 'n': 20, 'o': 21, 'p': 22, 'q': 23, 'r': 24, 's': 25, 't': 26, 'u': 27, 'z': 28, 'á': 29, 'é': 30, 'í': 31, 'ó': 32}\n",
      "{0: '\\n', 1: ' ', 2: '(', 3: ')', 4: ',', 5: '.', 6: 'A', 7: 'C', 8: 'I', 9: 'a', 10: 'b', 11: 'c', 12: 'd', 13: 'e', 14: 'f', 15: 'g', 16: 'h', 17: 'i', 18: 'l', 19: 'm', 20: 'n', 21: 'o', 22: 'p', 23: 'q', 24: 'r', 25: 's', 26: 't', 27: 'u', 28: 'z', 29: 'á', 30: 'é', 31: 'í', 32: 'ó'}\n"
     ]
    }
   ],
   "source": [
    "stoi = {ch:i for i,ch in enumerate(vocabulario)}\n",
    "print(stoi)\n",
    "itos = {i:ch for i,ch in enumerate(vocabulario)}\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokeniza(s):\n",
    "    ret = [stoi[c] for c in s]\n",
    "    return ret\n",
    "def destokeniza(ltokens):\n",
    "    ret = \"\".join([itos[i] for i in ltokens])\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22, 13, 22, 17, 26, 21]\n",
      "pepito\n"
     ]
    }
   ],
   "source": [
    "print(tokeniza(\"pepito\"))\n",
    "print(destokeniza([22, 13, 22, 17, 26, 21]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['¿Cuánto', 'tiempo', 'pasó', 'desde', 'que', 'comí', 'una', 'manzana?']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "tk = WhitespaceTokenizer()\n",
    "texto = \"¿Cuánto tiempo pasó desde que comí una manzana?\"\n",
    "texto_tokenizado = tk.tokenize(texto)\n",
    "print(texto_tokenizado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['¿', 'Cuánto', 'tiempo', 'pasó', 'desde', 'que', 'comí', 'una', 'manzana', '?']\n",
      "['¿Cuánto', 'tiempo', 'pasó', 'desde', 'que', 'comí', 'una', 'manzana', '?']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "texto = \"¿Cuánto tiempo pasó desde que comí una manzana?\"\n",
    "texto_tokenizado1 = WordPunctTokenizer().tokenize(texto)\n",
    "texto_tokenizado2 = TreebankWordTokenizer().tokenize(texto)\n",
    "print(texto_tokenizado1)\n",
    "print(texto_tokenizado2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'stanza'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\daniel.martinezcarre\\Desktop\\Repositorios\\ia_repositorio\\Programacion de IA\\tirmestre_2\\tokenizacion.ipynb Celda 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/daniel.martinezcarre/Desktop/Repositorios/ia_repositorio/Programacion%20de%20IA/tirmestre_2/tokenizacion.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mstanza\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/daniel.martinezcarre/Desktop/Repositorios/ia_repositorio/Programacion%20de%20IA/tirmestre_2/tokenizacion.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m stanza\u001b[39m.\u001b[39mdownload(\u001b[39m\"\u001b[39m\u001b[39mes\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/daniel.martinezcarre/Desktop/Repositorios/ia_repositorio/Programacion%20de%20IA/tirmestre_2/tokenizacion.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m nlp \u001b[39m=\u001b[39m stanza\u001b[39m.\u001b[39mPipeline(lang\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mes\u001b[39m\u001b[39m'\u001b[39m, processors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtokenize,mwt,pos,lemma\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'stanza'"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "stanza.download(\"es\")\n",
    "nlp = stanza.Pipeline(lang='es', processors='tokenize,mwt,pos,lemma')\n",
    "texto_pablito = \"Pablito clavó un clavito cuantos clavitos clava pablito\"\n",
    "doc = nlp(texto_pablito)\n",
    "print(*[f'Palabra: {word.text+\" \"}\\tLemma: {word.lemma}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token bueno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.trainers import BpeTrainer\n",
    "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "tokenizer.pre_tokenizer = Whitespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [f\"wikitext-103-raw-v1/wikitext-103-raw/wiki.{split}.raw\" for split in [\"test\", \"train\", \"valid\"]]\n",
    "tokenizer.train(files, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"tokenizer-wiki.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_file(\"tokenizer-wiki.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tokenizer.encode(\"Hello, y'all! How are you 😁 ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'y', \"'\", 'all', '!', 'How', 'are', 'you', '[UNK]', '?']\n"
     ]
    }
   ],
   "source": [
    "print(output.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27253, 16, 93, 11, 5097, 5, 7961, 5112, 6218, 0, 35]\n"
     ]
    }
   ],
   "source": [
    "print(output.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26, 27)\n"
     ]
    }
   ],
   "source": [
    "print(output.offsets[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'😁'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Hello, y'all! How are you 😁 ?\"\n",
    "sentence[26:27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.token_to_id(\"[SEP]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.processors import TemplateProcessing\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),\n",
    "        (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),\n",
    "    ],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
